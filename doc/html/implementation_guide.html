<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html>
<head>
<title>Infomap Implementation Guide</title>
</head>

<body>
<h1>Infomap Implementation Guide</h1>
<h2>Table of Contents</h2>
<ul>
  <li><a href="#intro">Introduction</a></li>
  <li><a href="#prep_progs">Table of <tt>preprocessing/</tt>
    Programs</a></li>
  <li><a href="#model_files">Table of Model Files</a></li>
</ul>


<h2><a name="intro"/>Introduction</h2>

<p>It is assumed that if you are reading this you have already figured
out how to install and run the Infomap code.  Instructions for doing
this are given in the <a href="user_manual.html">Infomap User
Manual</a>.  This document is intended to give you a more thorough
understanding of how the software is actually implemented; it should
help you if you are planning to extend or modify the code, or if you
are just curious about how it works.</p>

<p>The Infomap software has two fundamental functions: automatically
learning a vector-space model of the words and documents in a corpus,
and using the resulting model to perform search in which queries can
be used to retrieve matching words or matching documents.  We refer to
the code that performs the former of these functions, the learning, as
the <i>preprocessing code</i> or the <i>preprocessing-side code</i>.
We refer to the code that performs the latter function, the search, as
the <i>search code</i> or the <i>search-side code</i>.  Communication
between the preprocessing-side code and the search-side code happens
via a <i>model</i>, which is the object learned by the
preprocessing-side code.  In the current implementation, each model
consists of a collection of all the files in a single directory, the
<i>model directory</i> or <i>model data directory</i>.
</p>

<p>The Infomap software distribution, once unpacked from its
distributed form (as, <i>e.g.</i>, a tar file), consists of a single
top-level directory containing some files and some subdirectories.
The preprocessing-side code is found in the <tt>preprocessing/</tt>
and <tt>svd/</tt> directories; the search-side code is in the
<tt>search/</tt> directory.</p>

<p>Since a model is the means of communication between
preprocessing-side and search-side code, the names of the files making
up the model, the format of those files, and the semantics of each
file, must be agreed upon by the preprocessing-side and search-side
programs.  If changes are made to the names or contents of the files
making up a model, both the preprocessing-side and search-side code
will need to be recompiled, possibly after being modified to account
for the changes.  Furthermore, models built before these
changes may no longer be compatible with search-side code compiled
after the changes.</p>

<p>On the other hand, so long as the format of a model is
<i>not</i> changed, compatibility between the search-side code and
preprocessing-side code should be maintained.  This means, for one
thing, that multiple models can be built from different corpora (or
from the same corpus, using different parameters) only by changing the
appropriate variables in the top-level <tt>Makefile</tt>; these models
can be learned and used for search without the need to do any
recompilation.  Also, the algorithm used for learning could be altered
by making changes to the preprocessing-side code; so long as the new
algorithm still produced a model in the same format, the search-side
code would not need to be changed or recompiled.  Likewise, new
options or techniques could be added to the search-side code and the
modified code could still be used to search existing models.</p>

<p>All Infomap code refers to the files making up a model using
compiled-in constants that are <tt>#define</tt>'d in the file
<tt>filenames.h</tt> in the top-level directory.  This file is
automatically generated by <tt>make</tt> from the file
<tt>Makefile.filenames</tt>, also in the top-level directory.</p>

<p>The learning of a model is accomplished by a collection of programs
that run in series; each program produces output files that may be
used as input files by later programs.   The two tables below
describe, respectively, the preprocessing-side programs, listing the
input and output files for each and giving a brief description, and
the files making up a model, listing the program that produces each
and the programs that use it.  These tables are intended as a guide to
or framework for understanding the Infomap code.</p>


<h2><a name="prep_progs"/><tt>preprocessing/</tt> Programs</h2>
<p>
<table border="1">

<tr><th>Program</th><th>Input File(s)</th><th>Output File(s)</th>
    <th>Description</th>
</tr>

<tr><td><a name="prepare_corpus"/><b><tt>prepare_corpus</tt></b></td>
    <td>corpus</td>
    <td>
      <table border="0">
        <tr><td><tt><a href="#wordlist">wordlist</a></tt></td></tr>
	<tr><td><tt><a href="#dic">dic</a></tt></td></tr>
        <tr><td><tt><a href="#numFiles">numFiles</a></tt></td></tr>
	<tr><td><tt><a href="#number2name">number2name</a></tt></td></tr>
	<tr><td><tt><a
	href="#model_params.bin">model_params.bin</a></tt></td></tr>
	<tr><td><tt><a
	href="#model_info.bin">model_info.bin</a></tt></td></tr>
	<tr><td><tt><a
	href="#corpus_format.bin">corpus_format.bin</a></tt></td></tr>
      </table>
    </td>
    <td><p>This program tokenizes the corpus and writes it in a format
    that is more easily manipulated by other preprocessing-side
    programs.  It also writes a &quot;dictionary&quot; file
    (<tt>dic</tt>) listing the words encountered in the corpus and the
    term and document frequency of each; the list is sorted in
    decreasing order of term frequency.</p></td>
</tr>

<tr><td><a name="count_wordvec"/><b><tt>count_wordvec</tt></b></td>
    <td>
      <table border="0">
        <tr><td><tt><a href="#dic">dic</a></tt></td></tr>
	<tr><td><tt><a href="#numFiles">numFiles</a></tt></td></tr>
        <tr><td><tt><a href="#wordlist">wordlist</a></tt></td></tr>
	<tr><td><tt><a
	href="#model_params.bin">model_params.bin</a></tt></td></tr>
	<tr><td><tt><a
	href="#model_info.bin">model_info.bin</a></tt></td></tr>
      </table>
    </td>
    <td>
      <table border="0">
        <tr><td><tt><a href="#coll">coll</a></tt></td></tr>
	<tr><td><tt><a href="#indx">indx</a></tt></td></tr>
	<tr><td><tt><a href="#matlab">matlab</a></tt></td></tr>
	<tr><td><tt><a
	href="#model_params.bin">model_params.bin</a></tt></td></tr>
	<tr><td><tt><a 
	href="#model_info.bin">model_info.bin</a></tt></td></tr>
      </table>
    </td>
    <td><p>This program reads the tokenized corpus (<tt>wordlist</tt>)
    and associated dictionary (<tt>dic</tt>) and produces the
    co-occurence matrix that will later be reduced by SVD.  Part of
    this process is choosing which &quot;content-bearing&quot; words
    will be used as column labels.  (For more about content-bearing
    words, see the <a href="algorithm.html">algorithm
    description</a>.)</p>

    <p>The co-occurence matrix is output in a format suitable for use
    by SVDPACKC to perform SVD (the <tt>indx</tt> and <tt>coll</tt>
    files) and, optionally, in Matlab format (the <tt>matlab</tt>
    file) to enable hands-on manipulation.</p></td>
</tr>

<tr><td><a name="svdinterface"/><b><tt>../svd/svdinterface/svdinterface</tt></b></td>
    <td>
      <table border="0">
        <tr><td><tt><a href="#indx">indx</a></tt></td></tr>
	<tr><td><tt><a href="#coll">coll</a></tt></td></tr>
	<tr><td><tt><a
	href="#model_params.bin">model_params.bin</a></tt></td></tr>
	<tr><td><tt><a
	href="#model_info.bin">model_info.bin</a></tt></td></tr>
      </table>
    </td>
    <td>
      <table border="0">
        <tr><td><tt><a href="#left">left</a></tt></td></tr>
	<tr><td><tt><a href="#rght">rght</a></tt></td></tr>
	<tr><td><tt><a href="#sing">sing</a></tt></td></tr>
	<tr><td><tt><a
	href="#model_params.bin">model_params.bin</a></tt></td></tr>
	<tr><td><tt><a
	href="#model_info.bin">model_info.bin</a></tt></td></tr>
      </table>
    </td>
    <td><p>The <tt>svdinterface</tt> program (which is in the
    <tt>svd/svdinterface/</tt> subdirectory of the top-level Infomap
    directory, not in the <tt>preprocessing/</tt> subdirectory like
    all of the other programs discussed here) is a relatively simple
    wrapper for the SVDPACKC library from the University of Tennessee.
    The co-occurence matrix stored in the <tt>coll</tt> and
    <tt>indx</tt> files is reduced by SVD, producing the left singular
    vectors (<tt>left</tt>), right singular vectors (<tt>rght</tt>),
    and singular values (<tt>sing</tt>).  The left singular vectors
    are treated as the WordSpace word vectors, and will later be
    encoded by <tt>encode_wordvec</tt> into a more compact binary
    representation.  The right singular vectors (in the file
    <tt>rght</tt>) may or may not be preserved upon completion of the
    <tt>svdinterface</tt> program, depending on a <tt>Makefile</tt>
    variable.</p>

    <p>For a principled explanation of SVD and its use by the Infomap
    software, see the <a href="algorithm.html">algorithm
    description.</a></td>
</tr>

<tr><td><a name="encode_wordvec"/><b><tt>encode_wordvec</tt></b></td>
    <td>
      <table border="0">
        <tr><td><tt><a href="#left">left</a></tt></td></tr>
	<tr><td><tt><a href="#dic">dic</a></tt></td></tr>
	<tr><td><tt><a
	href="#model_params.bin">model_params.bin</a></tt></td></tr>
      </table>
    </td>
    <td>
      <table border="0">
        <tr><td><tt>
	  <a href="#wordvec.bin">wordvec.bin</a></tt></td></tr>
	<tr><td><tt>
	  <a href="#offset2word">offset2word</a></tt></td></tr>
	<tr><td><tt>
	  <a href="#word2offset">word2offset</a></tt></td></tr>
      </table>
    </td>
    <td><p>This program reads in the textual word vectors (in the file
    <tt>left</tt>) that were obtained using SVD by the
    <tt>svdinterface</tt> program, and writes them back out in a
    binary format to the file <tt>wordvec.bin</tt>.  This is a more
    compact, although less portable, represenation.  (The raw C
    numeric data is simply written to disk; this is <em>not</em>
    portable due to differences in hardware and compliers.)</p>
    
    <p>To enable lookup, two DBM databases are created.  The
    <tt>word2offset</tt> database allows the offset of a word's vector
    in the <tt>wordvec.bin</tt> file to be looked up using the word as
    a key.  This is used by the search code to obtain the vectors for
    the words in a query.  The <tt>offset2word</tt> database allows
    the inverse lookup: the offset of a word's vector in the
    <tt>wordvec.bin</tt> file can be used as a key to look up the
    word.  This is used by the search code to determine which words it
    should return once it has determined which word vectors are the
    best match for a query vector.
    </td>
</tr>

<tr><td><a name="count_artvec"/><b><tt>count_artvec</tt></b></td>
    <td>
      <table border="0">
        <tr><td><a href="#numFiles"><tt>numFiles</tt></a></td></tr>
        <tr><td><a href="#dic"><tt>dic</tt></a></td></tr>
	<tr><td><a href="#wordlist"><tt>wordlist</tt></a></td></tr>
        <tr><td><a
          href="#wordvec.bin"><tt>wordvec.bin</tt></a></td></tr> 
	<tr><td><tt><a
	href="#model_params.bin">model_params.bin</a></tt></td></tr>
      </table>
    </td>
    <td>
      <table border="0">
        <tr><td><tt>
	  <a href="#artvec.bin">artvec.bin</a></tt></td></tr>
	<tr><td><tt>
	  <a href="#offset2art">offset2art</a></tt></td></tr>
	<tr><td><tt>
	  <a href="#art2offset">art2offset</a></tt></td></tr>
      </table>
    </td>
    <td><p>This program uses the tokenized corpus (<tt>wordlist</tt>)
    and the word vectors already computed and stored in
    <tt>wordvec.bin</tt> to compute WordSpace vectors for the articles
    (documents) that make up the training corpus.  These vectors can
    be used by the search code for information retrieval:  those
    articles whose vectors most closely match the query vector are
    returned.  A binary representation of the article vectors is
    written to <tt>artvec.bin</tt> after these vectors are computed.x</p>

    <p>To enable lookup, two DBM databases are created.  The
    <tt>art2offset</tt> database allows the offset of an article's
    vector in the <tt>artvec.bin</tt> file to be looked up using the
    article's ID as a key.  The <tt>offset2art</tt> database allows
    the inverse lookup: the offset of an article's vector in the
    <tt>artvec.bin</tt> file can be used as a key to look up the
    article's ID.  This is used by the search code to determine which
    articles it should return once it has determined which article
    vectors are the best match for a query vector.</p>

    <p>In the case of single-file corpora, the article ID is the
    offset into the single corpus file at which the article begins.
    The end of the article is marked by a tag.  In the case of
    multiple-file corpora, the article ID is an index into the
    <tt>number2name</tt> DBM, which will map it to the filename of the
    file that consists entirely of the article.  This scheme explains
    why multi-file corpora must have a strict 1-1 document-file
    correspondence.</p>
    </td>
</tr>

<tr><td><a
name="write_text_params"/><b><tt>write_text_params</tt></b></td>
    <td>
      <table border="0">
	<tr><td><tt><a
	href="#model_params.bin">model_params.bin</a></tt></td></tr>
	<tr><td><tt><a
	href="#model_info.bin">model_info.bin</a></tt></td></tr>
	<tr><td><tt><a
	href="#corpus_format.bin">corpus_format.bin</a></tt></td></tr>
      </table>
    </td>
    <td>
      <table border="0">
	<tr><td><tt><a
	href="#model_params.txt">model_params.txt</a></tt></td></tr>
	<tr><td><tt><a
	href="#model_info.txt">model_info.txt</a></tt></td></tr>
	<tr><td><tt><a
	href="#corpus_format.txt">corpus_format.txt</a></tt></td></tr>
      </table>
    </td>
    <td><p>
    <tt>write_text_params</tt> translates the three binary parameters
    files into textual versions.  This allows easy visual inspection
    of these objects, and could enable their transfer to another
    machine, since the binary versions are not portable across architectures.
    </p></td>
</tr>

</table>
</p>

<h2><a name="model_files"/>Model Data Files</h2>

<table border="1">

<tr><th>Filename</th><th>Produced By</th>
    <th>Used By</th><th>Description</th>
</tr>

<tr><td><a name="art2offset"/><b><tt>art2offset</tt></b></td>
    <td><a href="#count_artvec"><tt>count_artvec</tt></a></td>
    <td></td>
    <td><p>A DBM file; actually two files (<tt>art2offset.dir</tt> and
      <tt>art2offset.pag</tt>).  Each key in this DB is an article ID; the
      corresponding value is the offset into <tt>artvec.bin</tt> at which
      the vector for that article can be found.</p></td>
</tr>

<tr><td><a name="artvec.bin"/><b><tt>artvec.bin</tt></b></td>
    <td><a href="#count_artvec"><tt>count_artvec</tt></a></td>
    <td></td>
    <td><p>This file contains the WordSpace vectors for the articles
      (documents) in the corpus from which this model was built.  To find
      the vector for a particular article, we look up that vector's offset
      using the article's ID in the <tt>art2offset</tt> DBM
      database.</p></td>
</tr>

<tr><td><a name="coll"/><b><tt>coll</tt></b></td>
    <td><a href="#count_wordvec"><tt>count_wordvec</tt></a></td>
    <td><a href="#svdinterface"><tt>svdinterface</tt></a></td>
    <td><p>This is one of two files representing the co-occurrence
    matrix in a format that can be read by the SVD code.  The other
    file is <a href="#indx"><tt>indx</tt></a>.</p></td>
</tr>

<tr><td><a name="dic"/><b><tt>dic</tt></b></td>
    <td><a href="#prepare_corpus"><tt>prepare_corpus</tt></a></td>
    <td>
      <table border="0">
        <tr><td>
	  <a href="#count_wordvec"><tt>count_wordvec</tt></a></td></tr>
	<tr><td>
	  <a href="#encode_wordvec"><tt>encode_wordvec</tt></a></td></tr>
	<tr><td>
	  <a href="#count_artvec"><tt>count_artvec</tt></a></td></tr>

      </table>
    </td>

    <td><p>The dictionary.  This text file lists all the types
      encountered in the corpus and their frequency.  The types are listed
      one per line, sorted in decreasing order of frequency.  Each line has
      the format:<pre> term_freq doc_freq is_stop word
      </pre>
      <tt>word</tt> is the word whose dictionary entry this is.
      <tt>term_freq</tt> is the number of times that <tt>word</tt> occurred
      in the training corpus.  <tt>doc_freq</tt> is the number of different
      documents (or articles) in the corpus in which <tt>word</tt>
      occurred.  <tt>is_stop</tt> has a non-zero value if <tt>word</tt> is a
      stopword (that is, if it appeared in the stoplist), and is <tt>0</tt> otherwise.
      </p></td>
</tr>

<tr><td><a name="indx"/><b><tt>indx</tt></b></td>
    <td><a href="#count_wordvec"><tt>count_wordvec</tt></a></td>
    <td><a href="#svdinterface"><tt>svdinterface</tt></a></td>
    <td><p>This is one of two files representing the co-occurrence
    matrix in a format that can be read by the SVD code.  The other
    file is <a href="#coll"><tt>coll</tt></a>.</p></td>
</tr>

<tr><td><a name="left"/><b><tt>left</tt></b></td>
    <td><a href="#svdinterface"><tt>svdinterface</tt></a></td>
    <td><a href="#encode_wordvec"><tt>encode_wordvec</tt></a></td>
    <td><p>The matrix of left singular vectors generated by SVD of the
    co-occurrence matrix.  These vectors are treated as the word
    vectors.</p></td>
</tr>

<tr><td><a name="matlab"/><b><tt>matlab</tt></b></td>
    <td><a href="#count_wordvec"><tt>count_wordvec</tt></a></td>
    <td></td>
    <td><p>This file is a representation of the co-occurrence matrix
    that is computed by <tt>count_wordvec</tt>, in a format that can
    be used as input by MATLAB.  (The same matrix is represented in a
    different format, used for SVD input, in the files <a
    href="#coll"><tt>coll</tt></a> and <a
    href="#indx"><tt>indx</tt></a>.)</p>

    <p>The generation of this file is optional, and is controlled by
    the <tt>Makefile</tt> variable <tt>WRITE_MATLAB_FORMAT</tt>.  It
    is not used by any of the Infomap software (either
    preprocessing-side or search-side), but you may find it useful for
    hands-on use in MATLAB, for instance to prototype new algorithms.</p></td>
</tr>

<tr><td><a name="matrix"/><b><tt>matrix</tt></b></td>
    <td></td>
    <td></td>
    <td><p></p></td>
</tr>

<tr><td><a name="model_params.bin"/><b><tt>model_params.bin</tt></b></td>
    <td><table border="0">
    <tr><td><a
    href="#prepare_corpus"><tt>prepare_corpus</tt></a></td></tr>
    <tr><td><a
    href="#count_wordvec"><tt>count_wordvec</tt></a></td></tr>
    <tr><td><a href="#svdinterface"><tt>svdinterface</tt></a></td></tr>
    </table></td>

    <td><table border="0">
    <tr><td><a
    href="#count_wordvec"><tt>count_wordvec</tt></a></td></tr>
    <tr><td><a
    href="#svdinterface"><tt>svdinterface</tt></a></td></tr>
    <tr><td><a
    href="#encode_wordvec"><tt>encode_wordvec</tt></a></td></tr>
    <tr><td><a
    href="#count_artvec"><tt>count_artvec</tt></a></td></tr>
    <tr><td><a href="#write_text_params"><tt>write_text_params</tt></a></td></tr>
    </table></td>
    <td><p>This file contains the parameters with which this model was
      built, in a binary format.  (It is a raw <tt>MODEL_PARAMS</tt>
      structure.)  This data can be manipulated using the functions declared
      in <tt>model_params.h</tt>.</p>

      <p>This file contains only those parameters that will be needed
      by search-side code.  Other parameters are instead stored in
      <tt>model_info.bin</tt>, so that <tt>model_params.bin</tt> will
      be small and fast to load.</p>
    </td>
</tr>

<tr><td><a name="model_params.txt"/><b><tt>model_params.txt</tt></b></td>
    <td><a href="#write_text_params"><tt>write_text_params</tt></a></td>
    <td></td>
    <td><p>The same information as <tt>model_params.bin</tt>, in a
    human-readable (and more portable) textual format.</p></td>
</tr>

<tr><td><a name="model_info.bin/"><b><tt>model_info.bin</tt></b></td>
    <td><table border="0">
    <tr><td><a
    href="#prepare_corpus"><tt>prepare_corpus</tt></a></td></tr>
    <tr><td><a
    href="#count_wordvec"><tt>count_wordvec</tt></a></td></tr>
    <tr><td><a href="#svdinterface"><tt>svdinterface</tt></a></td></tr>
    </table></td>

    <td><table border="0">
    <tr><td><a
    href="#count_wordvec"><tt>count_wordvec</tt></a></td></tr>
    <tr><td><a
    href="#svdinterface"><tt>svdinterface</tt></a></td></tr>
    <tr><td><a href="#write_text_params"><tt>write_text_params</tt></a></td></tr>
    </table></td>
    <td><p>Additional model parameters, beyond those stored in
    <tt>model_params.bin</tt></p></td>
</tr>

<tr><td><a name="model_info.txt/"><b><tt>model_info.txt</tt></b></td>
    <td><a href="#write_model_params"><tt>write_model_params</tt></a></td>
    <td></td>
    <td><p>The same information as <tt>model_info.bin</tt>, in a
    human-readable (and more portable) textual format.</p></td>
</tr>

<tr><td><a name="corpus_format.bin/"><b><tt>corpus_format.bin</tt></b></td>
    <td><a href="#prepare_corpus"><tt>prepare_corpus</tt></a></td>
    <td><a href="#write_text_params"><tt>write_text_params</tt></a></td>
    <td><p>Information about the input corpus format and how it was
    handled; for instance, the tags used to mark the beginning and end
    of documents, and XML/SGML character entities that have been stripped.
</tr>

<tr><td><a
name="corpus_format.txt/"><b><tt>corpus_format.txt</tt></b></td>
    <td><a href="#write_model_params"><tt>write_model_params</tt></a></td>
    <td></td>
    <td><p>The same information as <tt>corpus_format.bin</tt>, in a
    human-readable (and more portable) textual format.</p></td>
</tr>

<tr><td><a name="number2name"/><b><tt>number2name</tt></b></td>
    <td><a href="#prepare_corpus"><tt>prepare_corpus</tt></a></td>
    <td></td>
    <td><p>This is a DBM database (thus two actual files,
      <tt>number2name.dir</tt> and <tt>number2name.pag</tt>).  This
      database maps an article (document) number (ID) to the filename
      of the file containing the corresponding article.  It is
      relevant only to multiple-file corpora, and will only be
      produced for such corpora.</p>
      
      <p>Note that the files in a multiple-file corpus must each
      consist of exactly one corpus document (article).</p>
    </td>
</tr>

<tr><td><a name="numFiles"/><b><tt>numFiles</tt></b></td>
    <td><a href="#prepare_corpus"><tt>prepare_corpus</tt></a></td>
    <td><a href="#count_wordvec"><tt>count_wordvec</tt></a></td>
    <td><p>The number of different files that make up the
    corpus.</p></td>
</tr>

<tr><td><a name="offset2art"/><b><tt>offset2art</tt></b></td>
    <td><a href="#count_artvec"><tt>count_artvec</tt></a></td>
    <td></td>
    <td><p>This is a DBM database (thus two actual files,
      <tt>offset2art.dir</tt> and <tt>offset2art.pag</tt>).  This
      database maps the offset of an article (document) vector in
      <tt>artvec.bin</tt> to the article ID.</p></td>
</tr>

<tr><td><a name="offset2word"/><b><tt>offset2word</tt></b></td>
    <td><a href="#encode_wordvec"><tt>encode_wordvec</tt></a></td>
    <td></td>
    <td><p>This is a DBM database (thus two actual files,
      <tt>offset2word.dir</tt> and <tt>offset2word.pag</tt>).  This
      database maps the offset of a word vector in
      <tt>wordvec.bin</tt> to the word having that vector.</p></td>
</tr>

<tr><td><a name="rght"/><b><tt>rght</tt></b></td>
    <td><a href="#svdinterface"><tt>svdinterface</tt></a></td>
    <td></td>
    <td><p>The right singular vectors obtained during SVD.  These are
      not used by the search-side code, and their retention is
      optional (and controlled by the <tt>PRESERVE_RIGHT_SINGVECS</tt>
      variable in the <tt>Makefile</tt>).  For an explanation of SVD
      and its use by the Infomap software, see <a
      href="algorithm.html">this file</a>.</p></td>
</tr>

<tr><td><a name="sing"/><b><tt>sing</tt></b></td>
    <td><a href="#svdinterface"><tt>svdinterface</tt></a></td>
    <td></td>
    <td><p>The singular values produced by SVD of the co-occurrence
    matrix.  These are not currently used.</p></td>
</tr>

<tr><td><a name="word2offset"/><b><tt>word2offset</tt></b></td>
    <td><a href="#encode_wordvec"><tt>encode_wordvec</tt></a></td>
    <td></td>
    <td><p>This is a DBM database (thus two actual files,
      <tt>word2offset.dir</tt> and <tt>word2offset.pag</tt>).  This
      database maps a word to the offset of that word's vector in the
      <tt>wordvec.bin</tt> file.</p></td>
</tr>

<tr><td><a name="wordlist"/><b><tt>wordlist</tt></b></td>
    <td><tt><a href="#prepare_corpus">prepare_corpus</a></tt></td>
    <td>
      <table border="0">
        <tr><td><tt><a
	  href="#count_wordvec">count_wordvec</a></tt></td></tr>
	<tr><td><tt>
	  <a href="#count_artvec">count_artvec</a></tt></td></tr>
      </table>
    </td>
    <td><p>The tokenized corpus:  one token per line, plus some lines
    consisting of formatting information like markers for the
    beginning and end of documents, and document ID numbers.</p></td>
</tr>

<tr><td><a name="wordvec.bin"/><b><tt>wordvec.bin</tt></b></td>
    <td><tt><a href="#encode_wordvec">encode_wordvec</a></tt></td>
    <td><tt><a href="#count_artvec">count_artvec</a></tt></td>
    <td><p>This file contains the WordSpace vectors for the words in
      the corpus from which this model was built.  To find the vector
      for a particular word, we look up that vector's offset in this
      file using the word as a key into the <tt>word2offset</tt> DBM
      database.</p></td>
</tr>

</table>


</body>
</html>
